3:I[9275,[],""]
5:I[1343,[],""]
6:I[8700,["231","static/chunks/231-96154228ca59835c.js","564","static/chunks/564-355e63b9f56a3956.js","185","static/chunks/app/layout-4a2e59eaee55b818.js"],"ThemeProviders"]
7:I[4080,["231","static/chunks/231-96154228ca59835c.js","564","static/chunks/564-355e63b9f56a3956.js","185","static/chunks/app/layout-4a2e59eaee55b818.js"],""]
8:I[6787,["231","static/chunks/231-96154228ca59835c.js","564","static/chunks/564-355e63b9f56a3956.js","185","static/chunks/app/layout-4a2e59eaee55b818.js"],"KBarSearchProvider"]
9:I[231,["231","static/chunks/231-96154228ca59835c.js","173","static/chunks/173-d20cda91af84ebe4.js","49","static/chunks/app/posts/%5B...slug%5D/page-91554b8fd798c58d.js"],""]
a:I[1398,["231","static/chunks/231-96154228ca59835c.js","564","static/chunks/564-355e63b9f56a3956.js","185","static/chunks/app/layout-4a2e59eaee55b818.js"],"default"]
b:I[8976,["231","static/chunks/231-96154228ca59835c.js","564","static/chunks/564-355e63b9f56a3956.js","185","static/chunks/app/layout-4a2e59eaee55b818.js"],"default"]
4:["slug","fine-tuning-open-source-models-lessons-from-the-trenches","c"]
0:["X762CM9tBqan0vwJm_5vJ",[[["",{"children":["posts",{"children":[["slug","fine-tuning-open-source-models-lessons-from-the-trenches","c"],{"children":["__PAGE__?{\"slug\":[\"fine-tuning-open-source-models-lessons-from-the-trenches\"]}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["posts",{"children":[["slug","fine-tuning-open-source-models-lessons-from-the-trenches","c"],{"children":["__PAGE__",{},[["$L1","$L2"],null],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":[["$","link","0",{"rel":"stylesheet","href":"/blog/_next/static/css/e58252480e2fb6c1.css","precedence":"next","crossOrigin":"$undefined"}]]}],null]},["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","posts","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined","styles":null}],null]},[["$","html",null,{"lang":"en-us","className":"__variable_53d7e4 scroll-smooth","suppressHydrationWarning":true,"children":[["$","link",null,{"rel":"apple-touch-icon","sizes":"76x76","href":"/static/favicons/apple-touch-icon.png"}],["$","link",null,{"rel":"icon","type":"image/png","sizes":"32x32","href":"/static/favicons/favicon-32x32.png"}],["$","link",null,{"rel":"icon","type":"image/png","sizes":"16x16","href":"/static/favicons/favicon-16x16.png"}],["$","link",null,{"rel":"manifest","href":"/static/favicons/site.webmanifest"}],["$","link",null,{"rel":"mask-icon","href":"/static/favicons/safari-pinned-tab.svg","color":"#5bbad5"}],["$","meta",null,{"name":"msapplication-TileColor","content":"#000000"}],["$","meta",null,{"name":"theme-color","media":"(prefers-color-scheme: light)","content":"#fff"}],["$","meta",null,{"name":"theme-color","media":"(prefers-color-scheme: dark)","content":"#000"}],["$","link",null,{"rel":"alternate","type":"application/rss+xml","href":"/feed.xml"}],["$","body",null,{"className":"bg-white pl-[calc(100vw-100%)] text-black antialiased dark:bg-gray-950 dark:text-white","children":["$","$L6",null,{"children":[["$undefined","$undefined","$undefined","$undefined",[["$","$L7",null,{"strategy":"afterInteractive","src":"https://www.googletagmanager.com/gtag/js?id=UA-47059061-1"}],["$","$L7",null,{"strategy":"afterInteractive","id":"ga-script","children":"\n            window.dataLayer = window.dataLayer || [];\n            function gtag(){dataLayer.push(arguments);}\n            gtag('js', new Date());\n            gtag('config', 'UA-47059061-1');\n        "}]],"$undefined"],["$","section",null,{"className":"mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0","children":["$","div",null,{"className":"flex h-screen flex-col justify-between font-sans","children":[["$","$L8",null,{"kbarConfig":{"searchDocumentsPath":"search.json"},"children":[["$","header",null,{"className":"flex items-center justify-between py-10","children":[["$","div",null,{"children":["$","$L9",null,{"href":"/","aria-label":"Ryan Griego","children":["$","div",null,{"className":"flex items-center justify-between","children":[["$","div",null,{"className":"mr-3","children":["$","img",null,{"src":"https://res.cloudinary.com/dm7y3yvjp/image/upload/v1719071322/ryan-griego-blog-logo_nn6dgp.gif","alt":"Logo","width":"50","height":"50"}]}],["$","div",null,{"className":"hidden h-6 text-2xl font-semibold sm:block","children":"Ryan Griego"}]]}]}]}],["$","div",null,{"className":"flex items-center space-x-4 leading-5 sm:space-x-6","children":[[["$","$L9",null,{"href":"/posts","className":"hidden font-medium text-gray-900 dark:text-gray-100 sm:block","children":"Blog"}],["$","$L9",null,{"href":"/projects","className":"hidden font-medium text-gray-900 dark:text-gray-100 sm:block","children":"Projects"}],["$","$L9",null,{"href":"/about","className":"hidden font-medium text-gray-900 dark:text-gray-100 sm:block","children":"About"}],["$","a",null,{"target":"_blank","rel":"noopener noreferrer","href":"https://ryangriego.com/","className":"hidden font-medium text-gray-900 dark:text-gray-100 sm:block","children":"Portfolio"}]],["$","$La",null,{}],["$","$Lb",null,{}]]}]]}],["$","main",null,{"className":"mb-auto","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":["$","div",null,{"className":"flex flex-col items-start justify-start md:mt-24 md:flex-row md:items-center md:justify-center md:space-x-6","children":[["$","div",null,{"className":"space-x-2 pb-8 pt-6 md:space-y-5","children":["$","h1",null,{"className":"text-6xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 md:border-r-2 md:px-6 md:text-8xl md:leading-14","children":"404"}]}],["$","div",null,{"className":"max-w-md","children":[["$","p",null,{"className":"mb-4 text-xl font-bold leading-normal md:text-2xl","children":"Sorry we couldn't find this page."}],["$","p",null,{"className":"mb-8","children":"But dont worry, you can find plenty of other things on our homepage."}],["$","$L9",null,{"href":"/","className":"focus:shadow-outline-blue inline rounded-lg border border-transparent bg-blue-600 px-4 py-2 text-sm font-medium leading-5 text-white shadow transition-colors duration-150 hover:bg-blue-700 focus:outline-none dark:hover:bg-blue-500","children":"Back to homepage"}]]}]]}],"notFoundStyles":[],"styles":null}]}]]}],["$","footer",null,{"children":["$","div",null,{"className":"mt-16 flex flex-col items-center","children":[["$","div",null,{"className":"mb-3 flex space-x-4","children":[["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"mailto:ryangriego@gmail.com","children":[["$","span",null,{"className":"sr-only","children":"mail"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 20 20","className":"fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 dark:hover:text-primary-400 h-6 w-6","children":[["$","title",null,{"children":"Mail"}],["$","path",null,{"d":"M2.003 5.884L10 9.882l7.997-3.998A2 2 0 0016 4H4a2 2 0 00-1.997 1.884z"}],["$","path",null,{"d":"M18 8.118l-8 4-8-4V14a2 2 0 002 2h12a2 2 0 002-2V8.118z"}]]}]]}],["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://github.com/ryan-griego","children":[["$","span",null,{"className":"sr-only","children":"github"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 dark:hover:text-primary-400 h-6 w-6","children":[["$","title",null,{"children":"Github"}],["$","path",null,{"d":"M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"}]]}]]}],["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://youtube.com","children":[["$","span",null,{"className":"sr-only","children":"youtube"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 dark:hover:text-primary-400 h-6 w-6","children":[["$","title",null,{"children":"Youtube"}],["$","path",null,{"d":"M23.499 6.203a3.008 3.008 0 00-2.089-2.089c-1.87-.501-9.4-.501-9.4-.501s-7.509-.01-9.399.501a3.008 3.008 0 00-2.088 2.09A31.258 31.26 0 000 12.01a31.258 31.26 0 00.523 5.785 3.008 3.008 0 002.088 2.089c1.869.502 9.4.502 9.4.502s7.508 0 9.399-.502a3.008 3.008 0 002.089-2.09 31.258 31.26 0 00.5-5.784 31.258 31.26 0 00-.5-5.808zm-13.891 9.4V8.407l6.266 3.604z"}]]}]]}],["$","a",null,{"className":"text-sm text-gray-500 transition hover:text-gray-600","target":"_blank","rel":"noopener noreferrer","href":"https://www.linkedin.com/in/ryan-griego-2134a340/","children":[["$","span",null,{"className":"sr-only","children":"linkedin"}],["$","svg",null,{"xmlns":"http://www.w3.org/2000/svg","viewBox":"0 0 24 24","className":"fill-current text-gray-700 hover:text-primary-500 dark:text-gray-200 dark:hover:text-primary-400 h-6 w-6","children":[["$","title",null,{"children":"Linkedin"}],["$","path",null,{"d":"M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"}]]}]]}]]}],["$","div",null,{"className":"mb-2 flex space-x-2 text-sm text-gray-500 dark:text-gray-400","children":[["$","div",null,{"children":"Ryan Griego"}],["$","div",null,{"children":" • "}],["$","div",null,{"children":"© 2025"}]]}],["$","div",null,{"className":"mb-8 text-sm text-gray-500 dark:text-gray-400","children":["$","a",null,{"target":"_blank","rel":"noopener noreferrer","href":"https://github.com/ryan-griego/blog","children":"This is a Next.js Application hosted on a Digital Ocean Droplet"}]}]]}]}]]}]}]]}]}]]}],null],null],[[["$","link","0",{"rel":"stylesheet","href":"/blog/_next/static/css/b4197d88162948e5.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/blog/_next/static/css/338efafd8083b3dc.css","precedence":"next","crossOrigin":"$undefined"}]],"$Lc"]]]]
2:[["$","script",null,{"type":"application/ld+json","dangerouslySetInnerHTML":{"__html":"{\"@context\":\"https://schema.org\",\"@type\":\"BlogPosting\",\"headline\":\"Fine-Tuning Open Source Models: From the Trenches\",\"datePublished\":\"2025-08-18T07:00:00.000Z\",\"dateModified\":\"2025-08-18T07:00:00.000Z\",\"description\":\"A hands-on look at fine-tuning open source AI models using LoRA/QLoRA techniques on Amazon product data. I discovered how a fine-tuned 8B parameter model outperformed GPT-4o and Claude, while navigating the practical challenges of limited GPU resources and hyperparameter optimization.\",\"image\":\"/static/images/twitter-card.png\",\"url\":\"https://ryangriego.com/blog/posts/fine-tuning-open-source-models-lessons-from-the-trenches\",\"author\":[{\"@type\":\"Person\",\"name\":\"Ryan Griego\"}]}"}}],["$","section",null,{"className":"mx-auto max-w-3xl px-4 sm:px-6 xl:max-w-5xl xl:px-0","children":["$","article",null,{"children":["$","div",null,{"className":"xl:divide-y xl:divide-gray-200 xl:dark:divide-gray-700","children":[["$","header",null,{"className":"pt-6 xl:pb-6","children":["$","div",null,{"className":"space-y-1 text-center","children":[["$","dl",null,{"className":"space-y-10","children":["$","div",null,{"children":[["$","dt",null,{"className":"sr-only","children":"Published on"}],["$","dd",null,{"className":"text-base font-medium leading-6 text-gray-500 dark:text-gray-400","children":["$","time",null,{"dateTime":"2025-08-18T07:00:00.000Z","children":"Monday, August 18, 2025"}]}]]}]}],["$","div",null,{"children":["$","h1",null,{"className":"text-3xl font-extrabold leading-9 tracking-tight text-gray-900 dark:text-gray-100 sm:text-4xl sm:leading-10 md:text-5xl md:leading-14","children":"Fine-Tuning Open Source Models: From the Trenches"}]}]]}]}],["$","div",null,{"className":"grid-rows-[auto_1fr] divide-y divide-gray-200 pb-8 dark:divide-gray-700 xl:grid xl:grid-cols-4 xl:gap-x-6 xl:divide-y-0","children":[["$","dl",null,{"className":"pb-10 pt-6 xl:border-b xl:border-gray-200 xl:pt-11 xl:dark:border-gray-700","children":[["$","dt",null,{"className":"sr-only","children":"Authors"}],["$","dd",null,{"children":["$","ul",null,{"className":"flex flex-wrap justify-center gap-4 sm:space-x-12 xl:block xl:space-x-0 xl:space-y-8","children":[["$","li","Ryan Griego",{"className":"flex items-center space-x-2","children":[["$","img",null,{"src":"https://res.cloudinary.com/dm7y3yvjp/image/upload/v1719280168/ryan-blog-mug_qqosse.jpg","alt":"avatar","width":38,"height":38,"className":"h-10 w-10 rounded-full"}],["$","dl",null,{"className":"whitespace-nowrap text-sm font-medium leading-5","children":[["$","dt",null,{"className":"sr-only","children":"Name"}],["$","dd",null,{"className":"text-gray-900 dark:text-gray-100","children":"Ryan Griego"}],["$","dt",null,{"className":"sr-only","children":"Twitter"}],["$","dd",null,{"children":"$undefined"}]]}]]}]]}]}]]}],["$","div",null,{"className":"divide-y divide-gray-200 dark:divide-gray-700 xl:col-span-3 xl:row-span-2 xl:pb-0","children":["$","div",null,{"className":"prose max-w-none pb-8 pt-10 dark:prose-invert","children":[["$","h1",null,{"className":"content-header","id":"fine-tuning-open-source-models-lessons-from-the-trenches","children":[["$","a",null,{"href":"#fine-tuning-open-source-models-lessons-from-the-trenches","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Fine-Tuning Open Source Models: Lessons from the Trenches"]}],["$","p",null,{"children":"I'm now in the last 80% of the AI course I've been taking on Udemy. Lately, I've been wrestling with the section on fine-tuning an open source model. We've been using Google Colab to train various models from ChatGPT, Claude and open source models like Qwen."}],["$","h2",null,{"className":"content-header","id":"understanding-loraqlora-making-fine-tuning-accessible","children":[["$","a",null,{"href":"#understanding-loraqlora-making-fine-tuning-accessible","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Understanding LoRA/QLoRA: Making Fine-Tuning Accessible"]}],["$","p",null,{"children":"We've been using LoRA/QLoRA, a fine-tuning method which helps make fine-tuning faster and cheaper. Instead of updating all model parameters like in traditional fine-tuning, LoRA \"freezes\" the original weights and injects adapters into the network. These adapters greatly reduce the number of trainable parameters."}],["$","p",null,{"children":["The data we're working with is from ",["$","a",null,{"target":"_blank","rel":"noopener noreferrer","href":"https://huggingface.co/datasets","children":"Hugging Face"}]," and is comprised of thousands of Amazon product descriptions and prices."]}],["$","h2",null,{"className":"content-header","id":"david-vs-goliath-when-smaller-models-win","children":[["$","a",null,{"href":"#david-vs-goliath-when-smaller-models-win","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"David vs. Goliath: When Smaller Models Win"]}],["$","p",null,{"children":"What's been interesting about this part of the course has been plotting out the accuracy of various models after they have been fine-tuned and seeing how they compare. We are comparing the average dollar that the model is off in its prediction."}],["$","p",null,{"children":"For example, we let GPT-4o and Claude predict the prices of various products with their trillion+ parameters and were able to beat both with an 8 billion parameter model that was fine-tuned with the curated data. GPT-4o had a 76 dollar average error rate and the fine-tuned 8b model had a 47 dollar error rate."}],["$","h2",null,{"className":"content-header","id":"my-current-project-appliance-price-prediction","children":[["$","a",null,{"href":"#my-current-project-appliance-price-prediction","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"My Current Project: Appliance Price Prediction"]}],["$","p",null,{"children":"What I'm working on now is fine-tuning the Qwen2.5-7B model with a subset of the Amazon data that includes all appliance data. I want this model to be able to predict how much an appliance costs after the fine-tuning is done."}],["$","h2",null,{"className":"content-header","id":"the-reality-of-free-gpu-resources","children":[["$","a",null,{"href":"#the-reality-of-free-gpu-resources","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"The Reality of Free GPU Resources"]}],["$","p",null,{"children":"I've been using the Google Colab free plan with a T4 GPU to fine-tune the model and even with a training dataset of 25,000 items, I've found it to take too long. The free plan gives you a limited amount of compute time and shuts down before I've gotten through 10 percent of the data. This has given me the chance to tweak various fine-tuning hyperparameters that will help improve the speed and performance of the training."}],["$","h3",null,{"className":"content-header","id":"key-hyperparameters-ive-been-tweaking","children":[["$","a",null,{"href":"#key-hyperparameters-ive-been-tweaking","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Key Hyperparameters I've Been Tweaking"]}],["$","p",null,{"children":"A few of these parameters are:"}],["$","ul",null,{"children":[["$","li",null,{"children":[["$","strong",null,{"children":"Epochs"}],": The number of times the model iterates over the entire training set"]}],["$","li",null,{"children":[["$","strong",null,{"children":"Batch size"}],": Equal to the number of samples processed before the model updates its weights. Lowering the batch size value allowed the model to work more quickly in the resource-constrained environment I was using with the free plan"]}],["$","li",null,{"children":[["$","strong",null,{"children":"4-bit quantization"}],": A core feature of QLoRA that helps models be fine-tuned on limited hardware"]}]]}],["$","p",null,{"children":"Even after configuring these parameters to be more performant on the T4 GPU, I found myself deciding that I should go ahead and pay the $10 for 100 compute units and fine-tune the model on an A100 GPU."}],["$","h2",null,{"className":"content-header","id":"conclusion-the-final-project","children":[["$","a",null,{"href":"#conclusion-the-final-project","aria-hidden":"true","tabIndex":"-1","children":["$","span",null,{"className":"content-header-link","children":["$","svg",null,{"className":"h-5 linkicon w-5","fill":"currentColor","viewBox":"0 0 20 20","xmlns":"http://www.w3.org/2000/svg","children":[["$","path",null,{"d":"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z"}],["$","path",null,{"d":"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z"}]]}]}]}],"Conclusion: The Final Project"]}],["$","p",null,{"children":"Eventually, we will be using this fine-tuned model in our final project which is an autonomous multi-agent system. It made sense to me to produce a higher quality model with the pay-as-you-go plan."}],["$","p",null,{"children":"I'm very excited about this final project coming up and appreciate all the work it takes to get the model ready and connect everything together. The final project is a platform which can look for deals that have been published online, subscribe to RSS feeds so it can spot deals that have been published. When it finds a promising looking deal, it will read it, interpret it, and use a number of LLMs to make its own estimate of how much the product is worth. If it finds a good opportunity it will then automatically send a text message to me letting me know about the deal."}],["$","p",null,{"children":"It will work autonomously all the time and every so often I should receive a notification that the model determined that it found a deal worth looking into. There will be 7 agents collaborating (not all built on LLMs) to make this happen."}]]}]}],["$","footer",null,{"children":[["$","div",null,{"className":"divide-gray-200 text-sm font-medium leading-5 dark:divide-gray-700 xl:col-start-1 xl:row-start-2 xl:divide-y","children":[["$","div",null,{"className":"py-4 xl:py-8","children":[["$","h2",null,{"className":"text-xs uppercase tracking-wide text-gray-500 dark:text-gray-400","children":"Tags"}],["$","div",null,{"className":"flex flex-wrap","children":[["$","span",null,{"className":"mr-3 text-md font-medium uppercase text-primary-500","children":"AI"}],["$","span",null,{"className":"mr-3 text-md font-medium uppercase text-primary-500","children":"Python"}],["$","span",null,{"className":"mr-3 text-md font-medium uppercase text-primary-500","children":"Next.js"}],["$","span",null,{"className":"mr-3 text-md font-medium uppercase text-primary-500","children":"LangChain"}],["$","span",null,{"className":"mr-3 text-md font-medium uppercase text-primary-500","children":"OpenAI"}]]}]]}],["$","div",null,{"className":"flex justify-between py-4 xl:block xl:space-y-8 xl:py-8","children":[["$","div",null,{"children":[["$","h2",null,{"className":"text-xs uppercase tracking-wide text-gray-500 dark:text-gray-400","children":"Previous Article"}],["$","div",null,{"className":"text-primary-500 hover:text-primary-600 dark:hover:text-primary-400","children":["$","$L9",null,{"href":"/posts/buildling-my-first-rag-application-a-dive-into-AI-powered-student-enrollment","children":"Building My First RAG Application: A Dive into AI-Powered Student Enrollment"}]}]]}],["$","div",null,{"children":[["$","h2",null,{"className":"text-xs uppercase tracking-wide text-gray-500 dark:text-gray-400","children":"Next Article"}],["$","div",null,{"className":"text-primary-500 hover:text-primary-600 dark:hover:text-primary-400","children":["$","$L9",null,{"href":"/posts/building-the-price-is-right-an-autonomous-AI-deal-discovery-system","children":"Building The Price is Right: An Autonomous AI Deal Discovery System"}]}]]}]]}]]}],["$","div",null,{"className":"pt-4 xl:pt-8","children":["$","$L9",null,{"href":"/posts","className":"text-primary-500 hover:text-primary-600 dark:hover:text-primary-400","aria-label":"Back to the blog","children":"← Back to the blog"}]}]]}]]}]]}]}]}]]
c:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Fine-Tuning Open Source Models: From the Trenches | Ryan Griego's blog"}],["$","meta","3",{"name":"description","content":"A hands-on look at fine-tuning open source AI models using LoRA/QLoRA techniques on Amazon product data. I discovered how a fine-tuned 8B parameter model outperformed GPT-4o and Claude, while navigating the practical challenges of limited GPU resources and hyperparameter optimization."}],["$","meta","4",{"name":"robots","content":"index, follow"}],["$","meta","5",{"name":"googlebot","content":"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1"}],["$","link","6",{"rel":"canonical","href":"https://ryangriego.com/blog/posts/fine-tuning-open-source-models-lessons-from-the-trenches/"}],["$","link","7",{"rel":"alternate","type":"application/rss+xml","href":"https://ryangriego.com/blog/feed.xml/"}],["$","meta","8",{"property":"og:title","content":"Fine-Tuning Open Source Models: From the Trenches"}],["$","meta","9",{"property":"og:description","content":"A hands-on look at fine-tuning open source AI models using LoRA/QLoRA techniques on Amazon product data. I discovered how a fine-tuned 8B parameter model outperformed GPT-4o and Claude, while navigating the practical challenges of limited GPU resources and hyperparameter optimization."}],["$","meta","10",{"property":"og:url","content":"https://ryangriego.com/blog/posts/fine-tuning-open-source-models-lessons-from-the-trenches/"}],["$","meta","11",{"property":"og:site_name","content":"Ryan Griego's blog"}],["$","meta","12",{"property":"og:locale","content":"en_US"}],["$","meta","13",{"property":"og:image","content":"https://ryangriego.com/blog/static/images/twitter-card.png"}],["$","meta","14",{"property":"og:type","content":"article"}],["$","meta","15",{"property":"article:published_time","content":"2025-08-18T07:00:00.000Z"}],["$","meta","16",{"property":"article:modified_time","content":"2025-08-18T07:00:00.000Z"}],["$","meta","17",{"property":"article:author","content":"Ryan Griego"}],["$","meta","18",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","19",{"name":"twitter:title","content":"Fine-Tuning Open Source Models: From the Trenches"}],["$","meta","20",{"name":"twitter:description","content":"A hands-on look at fine-tuning open source AI models using LoRA/QLoRA techniques on Amazon product data. I discovered how a fine-tuned 8B parameter model outperformed GPT-4o and Claude, while navigating the practical challenges of limited GPU resources and hyperparameter optimization."}],["$","meta","21",{"name":"twitter:image","content":"https://ryangriego.com/blog/static/images/twitter-card.png"}],["$","meta","22",{"name":"next-size-adjust"}]]
1:null
