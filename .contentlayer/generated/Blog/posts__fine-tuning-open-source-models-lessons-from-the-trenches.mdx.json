{
  "title": "Fine-Tuning Open Source Models: From the Trenches",
  "date": "2025-08-18T07:00:00.000Z",
  "tags": [
    "AI",
    "Python",
    "Next.js",
    "LangChain",
    "OpenAI"
  ],
  "draft": false,
  "summary": "A hands-on look at fine-tuning open source AI models using LoRA/QLoRA techniques on Amazon product data. I discovered how a fine-tuned 8B parameter model outperformed GPT-4o and Claude, while navigating the practical challenges of limited GPU resources and hyperparameter optimization.",
  "body": {
    "raw": "# Fine-Tuning Open Source Models: Lessons from the Trenches\n\nI'm now in the last 80% of the AI course I've been taking on Udemy. Lately, I've been wrestling with the section on fine-tuning an open source model. We've been using Google Colab to train various models from ChatGPT, Claude and open source models like Qwen.\n\n## Understanding LoRA/QLoRA: Making Fine-Tuning Accessible\n\nWe've been using LoRA/QLoRA, a fine-tuning method which helps make fine-tuning faster and cheaper. Instead of updating all model parameters like in traditional fine-tuning, LoRA \"freezes\" the original weights and injects adapters into the network. These adapters greatly reduce the number of trainable parameters.\n\nThe data we're working with is from [Hugging Face](https://huggingface.co/datasets) and is comprised of thousands of Amazon product descriptions and prices.\n\n## David vs. Goliath: When Smaller Models Win\n\nWhat's been interesting about this part of the course has been plotting out the accuracy of various models after they have been fine-tuned and seeing how they compare. We are comparing the average dollar that the model is off in its prediction.\n\nFor example, we let GPT-4o and Claude predict the prices of various products with their trillion+ parameters and were able to beat both with an 8 billion parameter model that was fine-tuned with the curated data. GPT-4o had a 76 dollar average error rate and the fine-tuned 8b model had a 47 dollar error rate.\n\n## My Current Project: Appliance Price Prediction\n\nWhat I'm working on now is fine-tuning the Qwen2.5-7B model with a subset of the Amazon data that includes all appliance data. I want this model to be able to predict how much an appliance costs after the fine-tuning is done.\n\n## The Reality of Free GPU Resources\n\nI've been using the Google Colab free plan with a T4 GPU to fine-tune the model and even with a training dataset of 25,000 items, I've found it to take too long. The free plan gives you a limited amount of compute time and shuts down before I've gotten through 10 percent of the data. This has given me the chance to tweak various fine-tuning hyperparameters that will help improve the speed and performance of the training.\n\n### Key Hyperparameters I've Been Tweaking\n\nA few of these parameters are:\n\n- **Epochs**: The number of times the model iterates over the entire training set\n- **Batch size**: Equal to the number of samples processed before the model updates its weights. Lowering the batch size value allowed the model to work more quickly in the resource-constrained environment I was using with the free plan\n- **4-bit quantization**: A core feature of QLoRA that helps models be fine-tuned on limited hardware\n\nEven after configuring these parameters to be more performant on the T4 GPU, I found myself deciding that I should go ahead and pay the $10 for 100 compute units and fine-tune the model on an A100 GPU.\n\n## Conclusion: The Final Project\n\nEventually, we will be using this fine-tuned model in our final project which is an autonomous multi-agent system. It made sense to me to produce a higher quality model with the pay-as-you-go plan.\n\nI'm very excited about this final project coming up and appreciate all the work it takes to get the model ready and connect everything together. The final project is a platform which can look for deals that have been published online, subscribe to RSS feeds so it can spot deals that have been published. When it finds a promising looking deal, it will read it, interpret it, and use a number of LLMs to make its own estimate of how much the product is worth. If it finds a good opportunity it will then automatically send a text message to me letting me know about the deal.\n\nIt will work autonomously all the time and every so often I should receive a notification that the model determined that it found a deal worth looking into. There will be 7 agents collaborating (not all built on LLMs) to make this happen.\n",
    "code": "var Component=(()=>{var p=Object.create;var r=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,f=Object.prototype.hasOwnProperty;var w=(n,a)=>()=>(a||n((a={exports:{}}).exports,a),a.exports),v=(n,a)=>{for(var t in a)r(n,t,{get:a[t],enumerable:!0})},l=(n,a,t,o)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let i of u(a))!f.call(n,i)&&i!==t&&r(n,i,{get:()=>a[i],enumerable:!(o=m(a,i))||o.enumerable});return n};var b=(n,a,t)=>(t=n!=null?p(g(n)):{},l(a||!n||!n.__esModule?r(t,\"default\",{value:n,enumerable:!0}):t,n)),k=n=>l(r({},\"__esModule\",{value:!0}),n);var s=w((M,h)=>{h.exports=_jsx_runtime});var x={};v(x,{default:()=>c,frontmatter:()=>y});var e=b(s()),y={title:\"Fine-Tuning Open Source Models: From the Trenches\",date:\"2025-8-18\",tags:[\"AI\",\"Python\",\"Next.js\",\"LangChain\",\"OpenAI\"],draft:!1,summary:\"A hands-on look at fine-tuning open source AI models using LoRA/QLoRA techniques on Amazon product data. I discovered how a fine-tuned 8B parameter model outperformed GPT-4o and Claude, while navigating the practical challenges of limited GPU resources and hyperparameter optimization.\"};function d(n){let a={a:\"a\",h1:\"h1\",h2:\"h2\",h3:\"h3\",li:\"li\",p:\"p\",path:\"path\",span:\"span\",strong:\"strong\",svg:\"svg\",ul:\"ul\",...n.components};return(0,e.jsxs)(e.Fragment,{children:[(0,e.jsxs)(a.h1,{className:\"content-header\",id:\"fine-tuning-open-source-models-lessons-from-the-trenches\",children:[(0,e.jsx)(a.a,{href:\"#fine-tuning-open-source-models-lessons-from-the-trenches\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Fine-Tuning Open Source Models: Lessons from the Trenches\"]}),(0,e.jsx)(a.p,{children:\"I'm now in the last 80% of the AI course I've been taking on Udemy. Lately, I've been wrestling with the section on fine-tuning an open source model. We've been using Google Colab to train various models from ChatGPT, Claude and open source models like Qwen.\"}),(0,e.jsxs)(a.h2,{className:\"content-header\",id:\"understanding-loraqlora-making-fine-tuning-accessible\",children:[(0,e.jsx)(a.a,{href:\"#understanding-loraqlora-making-fine-tuning-accessible\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Understanding LoRA/QLoRA: Making Fine-Tuning Accessible\"]}),(0,e.jsx)(a.p,{children:`We've been using LoRA/QLoRA, a fine-tuning method which helps make fine-tuning faster and cheaper. Instead of updating all model parameters like in traditional fine-tuning, LoRA \"freezes\" the original weights and injects adapters into the network. These adapters greatly reduce the number of trainable parameters.`}),(0,e.jsxs)(a.p,{children:[\"The data we're working with is from \",(0,e.jsx)(a.a,{href:\"https://huggingface.co/datasets\",children:\"Hugging Face\"}),\" and is comprised of thousands of Amazon product descriptions and prices.\"]}),(0,e.jsxs)(a.h2,{className:\"content-header\",id:\"david-vs-goliath-when-smaller-models-win\",children:[(0,e.jsx)(a.a,{href:\"#david-vs-goliath-when-smaller-models-win\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"David vs. Goliath: When Smaller Models Win\"]}),(0,e.jsx)(a.p,{children:\"What's been interesting about this part of the course has been plotting out the accuracy of various models after they have been fine-tuned and seeing how they compare. We are comparing the average dollar that the model is off in its prediction.\"}),(0,e.jsx)(a.p,{children:\"For example, we let GPT-4o and Claude predict the prices of various products with their trillion+ parameters and were able to beat both with an 8 billion parameter model that was fine-tuned with the curated data. GPT-4o had a 76 dollar average error rate and the fine-tuned 8b model had a 47 dollar error rate.\"}),(0,e.jsxs)(a.h2,{className:\"content-header\",id:\"my-current-project-appliance-price-prediction\",children:[(0,e.jsx)(a.a,{href:\"#my-current-project-appliance-price-prediction\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"My Current Project: Appliance Price Prediction\"]}),(0,e.jsx)(a.p,{children:\"What I'm working on now is fine-tuning the Qwen2.5-7B model with a subset of the Amazon data that includes all appliance data. I want this model to be able to predict how much an appliance costs after the fine-tuning is done.\"}),(0,e.jsxs)(a.h2,{className:\"content-header\",id:\"the-reality-of-free-gpu-resources\",children:[(0,e.jsx)(a.a,{href:\"#the-reality-of-free-gpu-resources\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"The Reality of Free GPU Resources\"]}),(0,e.jsx)(a.p,{children:\"I've been using the Google Colab free plan with a T4 GPU to fine-tune the model and even with a training dataset of 25,000 items, I've found it to take too long. The free plan gives you a limited amount of compute time and shuts down before I've gotten through 10 percent of the data. This has given me the chance to tweak various fine-tuning hyperparameters that will help improve the speed and performance of the training.\"}),(0,e.jsxs)(a.h3,{className:\"content-header\",id:\"key-hyperparameters-ive-been-tweaking\",children:[(0,e.jsx)(a.a,{href:\"#key-hyperparameters-ive-been-tweaking\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Key Hyperparameters I've Been Tweaking\"]}),(0,e.jsx)(a.p,{children:\"A few of these parameters are:\"}),(0,e.jsxs)(a.ul,{children:[(0,e.jsxs)(a.li,{children:[(0,e.jsx)(a.strong,{children:\"Epochs\"}),\": The number of times the model iterates over the entire training set\"]}),(0,e.jsxs)(a.li,{children:[(0,e.jsx)(a.strong,{children:\"Batch size\"}),\": Equal to the number of samples processed before the model updates its weights. Lowering the batch size value allowed the model to work more quickly in the resource-constrained environment I was using with the free plan\"]}),(0,e.jsxs)(a.li,{children:[(0,e.jsx)(a.strong,{children:\"4-bit quantization\"}),\": A core feature of QLoRA that helps models be fine-tuned on limited hardware\"]})]}),(0,e.jsx)(a.p,{children:\"Even after configuring these parameters to be more performant on the T4 GPU, I found myself deciding that I should go ahead and pay the $10 for 100 compute units and fine-tune the model on an A100 GPU.\"}),(0,e.jsxs)(a.h2,{className:\"content-header\",id:\"conclusion-the-final-project\",children:[(0,e.jsx)(a.a,{href:\"#conclusion-the-final-project\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Conclusion: The Final Project\"]}),(0,e.jsx)(a.p,{children:\"Eventually, we will be using this fine-tuned model in our final project which is an autonomous multi-agent system. It made sense to me to produce a higher quality model with the pay-as-you-go plan.\"}),(0,e.jsx)(a.p,{children:\"I'm very excited about this final project coming up and appreciate all the work it takes to get the model ready and connect everything together. The final project is a platform which can look for deals that have been published online, subscribe to RSS feeds so it can spot deals that have been published. When it finds a promising looking deal, it will read it, interpret it, and use a number of LLMs to make its own estimate of how much the product is worth. If it finds a good opportunity it will then automatically send a text message to me letting me know about the deal.\"}),(0,e.jsx)(a.p,{children:\"It will work autonomously all the time and every so often I should receive a notification that the model determined that it found a deal worth looking into. There will be 7 agents collaborating (not all built on LLMs) to make this happen.\"})]})}function c(n={}){let{wrapper:a}=n.components||{};return a?(0,e.jsx)(a,{...n,children:(0,e.jsx)(d,{...n})}):d(n)}return k(x);})();\n;return Component;"
  },
  "_id": "posts/fine-tuning-open-source-models-lessons-from-the-trenches.mdx",
  "_raw": {
    "sourceFilePath": "posts/fine-tuning-open-source-models-lessons-from-the-trenches.mdx",
    "sourceFileName": "fine-tuning-open-source-models-lessons-from-the-trenches.mdx",
    "sourceFileDir": "posts",
    "contentType": "mdx",
    "flattenedPath": "posts/fine-tuning-open-source-models-lessons-from-the-trenches"
  },
  "type": "Blog",
  "readingTime": {
    "text": "4 min read",
    "minutes": 3.345,
    "time": 200700,
    "words": 669
  },
  "slug": "fine-tuning-open-source-models-lessons-from-the-trenches",
  "path": "posts/fine-tuning-open-source-models-lessons-from-the-trenches",
  "filePath": "posts/fine-tuning-open-source-models-lessons-from-the-trenches.mdx",
  "toc": [
    {
      "value": "Fine-Tuning Open Source Models: Lessons from the Trenches",
      "url": "#fine-tuning-open-source-models-lessons-from-the-trenches",
      "depth": 1
    },
    {
      "value": "Understanding LoRA/QLoRA: Making Fine-Tuning Accessible",
      "url": "#understanding-loraqlora-making-fine-tuning-accessible",
      "depth": 2
    },
    {
      "value": "David vs. Goliath: When Smaller Models Win",
      "url": "#david-vs-goliath-when-smaller-models-win",
      "depth": 2
    },
    {
      "value": "My Current Project: Appliance Price Prediction",
      "url": "#my-current-project-appliance-price-prediction",
      "depth": 2
    },
    {
      "value": "The Reality of Free GPU Resources",
      "url": "#the-reality-of-free-gpu-resources",
      "depth": 2
    },
    {
      "value": "Key Hyperparameters I've Been Tweaking",
      "url": "#key-hyperparameters-ive-been-tweaking",
      "depth": 3
    },
    {
      "value": "Conclusion: The Final Project",
      "url": "#conclusion-the-final-project",
      "depth": 2
    }
  ],
  "structuredData": {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Fine-Tuning Open Source Models: From the Trenches",
    "datePublished": "2025-08-18T07:00:00.000Z",
    "dateModified": "2025-08-18T07:00:00.000Z",
    "description": "A hands-on look at fine-tuning open source AI models using LoRA/QLoRA techniques on Amazon product data. I discovered how a fine-tuned 8B parameter model outperformed GPT-4o and Claude, while navigating the practical challenges of limited GPU resources and hyperparameter optimization.",
    "image": "/static/images/twitter-card.png",
    "url": "https://ryangriego.com/blog/posts/fine-tuning-open-source-models-lessons-from-the-trenches"
  }
}