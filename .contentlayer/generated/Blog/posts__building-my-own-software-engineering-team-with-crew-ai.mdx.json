{
  "title": "Building My Own Software Engineering Team with CrewAI",
  "date": "2025-12-09T00:00:00.000Z",
  "tags": [
    "AI",
    "Python",
    "CrewAI",
    "Ollama",
    "Gradio",
    "Docker",
    "Multi-Agent",
    "LLM"
  ],
  "draft": false,
  "summary": "How I built an AI-powered engineering team that assembles backend, frontend, and test modules on demand‚Äîusing local LLMs with Ollama, dynamic templates, and robust Docker sandboxing.",
  "body": {
    "raw": "\n*A practical guide to creating a fully autonomous, locally-run multi-agent engineering team that builds custom Python+Gradio applications using open-source AI models.*\n\n---\n\n<div style={{textAlign: 'center', margin: '2rem 0', padding: '1.5rem', backgroundColor: '#f8f9fa', borderRadius: '8px', border: '2px solid #e9ecef'}}>\n  <h3 style={{margin: '0 0 1rem 0', color: '#495057'}}>üõ†Ô∏è Try the Engineering Team Generator</h3>\n  <p style={{margin: '0 0 1rem 0', color: '#6c757d'}}>Spin up your own application-engineering team, powered by local AI</p>\n  <a href=\"https://github.com/ryan-griego/engineering_team_v1\" target=\"_blank\" rel=\"noopener noreferrer\" style={{display: 'inline-block', backgroundColor: '#007bff', color: 'white', padding: '12px 24px', textDecoration: 'none', borderRadius: '6px', fontWeight: '600'}}>Get the Code ‚Üí</a>\n</div>\n\n## The Challenge\n\nSoftware development often demands an entire team: designers, backend engineers, frontend builders, and testers. What if you could automate that flow‚Äîusing local, private LLMs‚Äîto produce production-ready Python apps with zero cloud costs and complete sandboxing?\n\nThe goal: deliver an open-source, privacy-respecting system that orchestrates AI \"team members\" collaborating to take requirements, design backend APIs, generate a Gradio UI, and write tests, all powered by your own hardware.\n\n![Terminal output showing successful crew execution with GPT models](https://res.cloudinary.com/dm7y3yvjp/image/upload/v1765322028/CrewAI-engineering-team_hrqmcv.png)\n\n## Why Local and Multi-Agent?\n- **Cost & Privacy:** Run open models (like Llama 3) fully offline with Ollama‚Äîno API keys, no data sharing.\n- **Real Team Dynamics:** Use CrewAI to simulate a real-life SDLC: each agent has a role, voice, and outputs files a human could.\n- **Code Safety:** All code execution (for both dev and tests) is sandboxed with Docker, separating AI from your workstation and data.\n\n## The Solution\n\n### 1. CrewAI Multi-Agent Coordination\nInstead of a single monolithic LLM prompt, we leverage [CrewAI](https://github.com/joaomdmoura/CrewAI) to define agents (lead, backend, frontend, QA). Each agent has custom YAML config for its goals and outputs.\n\n### 2. Local LLMs via Ollama\nAgents use [Ollama](https://ollama.com/) as their LLM gateway, running models like `llama3.2:latest` or `llama3.1:8b` on your machine. Switch models at any time in YAML: pay nothing and keep your code/data private.\n\n### 3. Interactive Requirement Selection\nA new, user-friendly selector lets you pick (or write) your app spec at launch. Output folders are auto-named‚Äîeasy to organize and run multiple apps side-by-side.\n\n### 4. Dynamic Output Generation\nEach agent writes its deliverable into a dynamic output folder: design doc, backend module, UI (`app.py`), and tests. All code is ready to run or edit immediately.\n\n### 5. Dockerized Code Execution\nThe backend and test agents can execute code in a Docker container for complete safety, ensuring untrusted code never touches your real machine.\n\n## Key Features\n- **Multi-Agent Orchestration:** division of roles, clear system boundaries, explicit task handoffs.\n- **YAML/ENV Based Config:** All agents, tasks, and environment (LLM, Docker, etc) are user-tunable.\n- **Locally-Sourced AI:** Use modern open-source models without the cloud or usage quotas.\n- **Instant UI Prototypes:** Gradio UIs are generated on the fly and launch-ready.\n- **Safe By Default:** Docker execution makes AI-powered codegen safe for rapid experimentation.\n\n## Real-World Experience: Model Performance & Challenges\n\nDuring development, I extensively tested this system across multiple model configurations to understand the tradeoffs between local and cloud-based LLMs.\n\n### Model Testing & Comparison\n\nI experimented with both local Ollama models and OpenAI's GPT models:\n- **Local models:** `llama3.2:latest` and `llama3.1:8b`\n- **Cloud models:** `gpt-4o-mini` and `gpt-4o`\n\nWhile the appeal of local models is undeniable‚Äîzero cost, complete privacy, and no rate limits‚Äîthe reality proved more complex in practice.\n\n### The Code Interpreter Challenge\n\nThe most significant issue I encountered was with tool calling, specifically the Code Interpreter tool that validates generated Python code within the Docker sandbox. This tool is critical for ensuring the AI-generated code actually works before committing it to the output folder.\n\n**The problem:** Across my testing sessions, the Code Interpreter experienced 24 failed execution attempts when using local Llama models. These failures stemmed from various issues:\n- Syntax errors in generated code that the local models didn't catch\n- Dependency misconfiguration (missing imports, incorrect package versions)\n- Docker connection issues triggered by malformed execution commands\n- Edge cases in code generation that produced valid-looking but non-functional Python\n\n\n\n### Quality Differences: Local vs Cloud\n\nAfter extensive testing, I consistently found that **GPT-4o and GPT-4o-mini produced significantly higher-quality output projects**. The differences were notable across multiple dimensions:\n\n**Code Quality:**\n- GPT models generated cleaner, more idiomatic Python\n- Better adherence to the specified architecture patterns\n- Fewer syntax errors and runtime exceptions\n\n**Tool Calling Reliability:**\n- GPT models successfully used the Code Interpreter tool with minimal failures\n- More accurate Docker command generation\n- Better understanding of when to validate code vs when to proceed\n\n**Project Completeness:**\n- GPT-generated projects required fewer manual fixes\n- More comprehensive test coverage\n- Better integration between backend, frontend, and test modules\n\n### The Tradeoff Decision\n\nThis experience highlighted a fundamental tradeoff in AI-powered development tools:\n\n**Local Models (Llama):**\n- ‚úÖ Zero cost, complete privacy\n- ‚úÖ No rate limits or API dependencies\n- ‚úÖ Fast iteration for simple tasks\n- ‚ùå Higher failure rates on complex tool usage\n- ‚ùå More manual intervention required\n\n**Cloud Models (GPT-4o/4o-mini):**\n- ‚úÖ Superior code quality and reliability\n- ‚úÖ Successful tool calling and validation\n- ‚úÖ More production-ready outputs\n- ‚ùå API costs (though 4o-mini is very affordable)\n- ‚ùå Requires internet connection and API keys\n\nFor rapid prototyping and experimentation, I found the best workflow was to use GPT-4o-mini as the default, with the option to fall back to local models for privacy-sensitive projects or when iterating on prompts and configurations.\n\n## Lessons Learned\n\n- **Model Selection Matters:** While Llama 3.1 8B offers decent accuracy for straightforward tasks, the quality gap becomes apparent in complex multi-agent scenarios. GPT-4o excels at tool calling and produces more reliable code, while 4o-mini offers an excellent balance of quality and cost. Local models remain valuable for privacy-critical work, but expect to invest more time in validation and debugging.\n\n- **Tool Calling Is the Bottleneck:** The Code Interpreter tool is essential for validating AI-generated code, but it's also the most failure-prone component when using smaller or less capable models. Robust error handling and retry logic are critical.\n\n- **Environment Setup = Success:** Docker sockets and .env management are crucial for safe code execution and reproducibility. One misconfigured environment variable can cascade into dozens of failed tool calls.\n\n- **Iterate and Refine:** Open-source projects thrive when every config, error, and user workflow is simplified for real devs. The flexibility to swap between local and cloud models made this project practical for different use cases.\n\n## Technical Stack Summary\n\n**AI Models:** llama3.2:latest, llama3.1:8b (Ollama local), gpt-4o-mini, gpt-4o (OpenAI cloud)\n\n**Frameworks:** CrewAI, Gradio\n\n**Infrastructure:** Ollama (local LLM server), Docker (code sandbox), Python 3.12+\n\n**Languages:** Python\n\n**Key Libraries:** CrewAI[tools], Gradio, python-dotenv, pyyaml\n\n**Config:** YAML, .env\n\n---\n\n## Sources\n- [GitHub: engineering_team_v1](https://github.com/ryan-griego/engineering_team_v1)\n- [CrewAI](https://github.com/joaomdmoura/CrewAI)\n- [Ollama](https://ollama.com/)\n- [Gradio](https://www.gradio.app/)\n- [Docker](https://www.docker.com/)\n",
    "code": "var Component=(()=>{var m=Object.create;var i=Object.defineProperty;var p=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,w=Object.prototype.hasOwnProperty;var f=(a,n)=>()=>(n||a((n={exports:{}}).exports,n),n.exports),v=(a,n)=>{for(var l in n)i(a,l,{get:n[l],enumerable:!0})},o=(a,n,l,t)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let r of u(n))!w.call(a,r)&&r!==l&&i(a,r,{get:()=>n[r],enumerable:!(t=p(n,r))||t.enumerable});return a};var y=(a,n,l)=>(l=a!=null?m(g(a)):{},o(n||!a||!a.__esModule?i(l,\"default\",{value:a,enumerable:!0}):l,a)),x=a=>o(i({},\"__esModule\",{value:!0}),a);var c=f((M,d)=>{d.exports=_jsx_runtime});var b={};v(b,{default:()=>h,frontmatter:()=>k});var e=y(c()),k={title:\"Building My Own Software Engineering Team with CrewAI\",date:\"2025-12-09\",tags:[\"AI\",\"Python\",\"CrewAI\",\"Ollama\",\"Gradio\",\"Docker\",\"Multi-Agent\",\"LLM\"],draft:!1,summary:\"How I built an AI-powered engineering team that assembles backend, frontend, and test modules on demand\\u2014using local LLMs with Ollama, dynamic templates, and robust Docker sandboxing.\"};function s(a){let n={a:\"a\",code:\"code\",em:\"em\",h2:\"h2\",h3:\"h3\",hr:\"hr\",img:\"img\",li:\"li\",p:\"p\",path:\"path\",span:\"span\",strong:\"strong\",svg:\"svg\",ul:\"ul\",...a.components};return(0,e.jsxs)(e.Fragment,{children:[(0,e.jsx)(n.p,{children:(0,e.jsx)(n.em,{children:\"A practical guide to creating a fully autonomous, locally-run multi-agent engineering team that builds custom Python+Gradio applications using open-source AI models.\"})}),(0,e.jsx)(n.hr,{}),(0,e.jsxs)(\"div\",{style:{textAlign:\"center\",margin:\"2rem 0\",padding:\"1.5rem\",backgroundColor:\"#f8f9fa\",borderRadius:\"8px\",border:\"2px solid #e9ecef\"},children:[(0,e.jsx)(\"h3\",{style:{margin:\"0 0 1rem 0\",color:\"#495057\"},children:\"\\u{1F6E0}\\uFE0F Try the Engineering Team Generator\"}),(0,e.jsx)(\"p\",{style:{margin:\"0 0 1rem 0\",color:\"#6c757d\"},children:\"Spin up your own application-engineering team, powered by local AI\"}),(0,e.jsx)(\"a\",{href:\"https://github.com/ryan-griego/engineering_team_v1\",target:\"_blank\",rel:\"noopener noreferrer\",style:{display:\"inline-block\",backgroundColor:\"#007bff\",color:\"white\",padding:\"12px 24px\",textDecoration:\"none\",borderRadius:\"6px\",fontWeight:\"600\"},children:\"Get the Code \\u2192\"})]}),(0,e.jsxs)(n.h2,{className:\"content-header\",id:\"the-challenge\",children:[(0,e.jsx)(n.a,{href:\"#the-challenge\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"The Challenge\"]}),(0,e.jsx)(n.p,{children:\"Software development often demands an entire team: designers, backend engineers, frontend builders, and testers. What if you could automate that flow\\u2014using local, private LLMs\\u2014to produce production-ready Python apps with zero cloud costs and complete sandboxing?\"}),(0,e.jsx)(n.p,{children:'The goal: deliver an open-source, privacy-respecting system that orchestrates AI \"team members\" collaborating to take requirements, design backend APIs, generate a Gradio UI, and write tests, all powered by your own hardware.'}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.img,{alt:\"Terminal output showing successful crew execution with GPT models\",src:\"https://res.cloudinary.com/dm7y3yvjp/image/upload/v1765322028/CrewAI-engineering-team_hrqmcv.png\"})}),(0,e.jsxs)(n.h2,{className:\"content-header\",id:\"why-local-and-multi-agent\",children:[(0,e.jsx)(n.a,{href:\"#why-local-and-multi-agent\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Why Local and Multi-Agent?\"]}),(0,e.jsxs)(n.ul,{children:[(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Cost & Privacy:\"}),\" Run open models (like Llama 3) fully offline with Ollama\\u2014no API keys, no data sharing.\"]}),(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Real Team Dynamics:\"}),\" Use CrewAI to simulate a real-life SDLC: each agent has a role, voice, and outputs files a human could.\"]}),(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Code Safety:\"}),\" All code execution (for both dev and tests) is sandboxed with Docker, separating AI from your workstation and data.\"]})]}),(0,e.jsxs)(n.h2,{className:\"content-header\",id:\"the-solution\",children:[(0,e.jsx)(n.a,{href:\"#the-solution\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"The Solution\"]}),(0,e.jsxs)(n.h3,{className:\"content-header\",id:\"1-crewai-multi-agent-coordination\",children:[(0,e.jsx)(n.a,{href:\"#1-crewai-multi-agent-coordination\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"1. CrewAI Multi-Agent Coordination\"]}),(0,e.jsxs)(n.p,{children:[\"Instead of a single monolithic LLM prompt, we leverage \",(0,e.jsx)(n.a,{href:\"https://github.com/joaomdmoura/CrewAI\",children:\"CrewAI\"}),\" to define agents (lead, backend, frontend, QA). Each agent has custom YAML config for its goals and outputs.\"]}),(0,e.jsxs)(n.h3,{className:\"content-header\",id:\"2-local-llms-via-ollama\",children:[(0,e.jsx)(n.a,{href:\"#2-local-llms-via-ollama\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"2. Local LLMs via Ollama\"]}),(0,e.jsxs)(n.p,{children:[\"Agents use \",(0,e.jsx)(n.a,{href:\"https://ollama.com/\",children:\"Ollama\"}),\" as their LLM gateway, running models like \",(0,e.jsx)(n.code,{children:\"llama3.2:latest\"}),\" or \",(0,e.jsx)(n.code,{children:\"llama3.1:8b\"}),\" on your machine. Switch models at any time in YAML: pay nothing and keep your code/data private.\"]}),(0,e.jsxs)(n.h3,{className:\"content-header\",id:\"3-interactive-requirement-selection\",children:[(0,e.jsx)(n.a,{href:\"#3-interactive-requirement-selection\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"3. Interactive Requirement Selection\"]}),(0,e.jsx)(n.p,{children:\"A new, user-friendly selector lets you pick (or write) your app spec at launch. Output folders are auto-named\\u2014easy to organize and run multiple apps side-by-side.\"}),(0,e.jsxs)(n.h3,{className:\"content-header\",id:\"4-dynamic-output-generation\",children:[(0,e.jsx)(n.a,{href:\"#4-dynamic-output-generation\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"4. Dynamic Output Generation\"]}),(0,e.jsxs)(n.p,{children:[\"Each agent writes its deliverable into a dynamic output folder: design doc, backend module, UI (\",(0,e.jsx)(n.code,{children:\"app.py\"}),\"), and tests. All code is ready to run or edit immediately.\"]}),(0,e.jsxs)(n.h3,{className:\"content-header\",id:\"5-dockerized-code-execution\",children:[(0,e.jsx)(n.a,{href:\"#5-dockerized-code-execution\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"5. Dockerized Code Execution\"]}),(0,e.jsx)(n.p,{children:\"The backend and test agents can execute code in a Docker container for complete safety, ensuring untrusted code never touches your real machine.\"}),(0,e.jsxs)(n.h2,{className:\"content-header\",id:\"key-features\",children:[(0,e.jsx)(n.a,{href:\"#key-features\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Key Features\"]}),(0,e.jsxs)(n.ul,{children:[(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Multi-Agent Orchestration:\"}),\" division of roles, clear system boundaries, explicit task handoffs.\"]}),(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"YAML/ENV Based Config:\"}),\" All agents, tasks, and environment (LLM, Docker, etc) are user-tunable.\"]}),(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Locally-Sourced AI:\"}),\" Use modern open-source models without the cloud or usage quotas.\"]}),(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Instant UI Prototypes:\"}),\" Gradio UIs are generated on the fly and launch-ready.\"]}),(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Safe By Default:\"}),\" Docker execution makes AI-powered codegen safe for rapid experimentation.\"]})]}),(0,e.jsxs)(n.h2,{className:\"content-header\",id:\"real-world-experience-model-performance--challenges\",children:[(0,e.jsx)(n.a,{href:\"#real-world-experience-model-performance--challenges\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Real-World Experience: Model Performance & Challenges\"]}),(0,e.jsx)(n.p,{children:\"During development, I extensively tested this system across multiple model configurations to understand the tradeoffs between local and cloud-based LLMs.\"}),(0,e.jsxs)(n.h3,{className:\"content-header\",id:\"model-testing--comparison\",children:[(0,e.jsx)(n.a,{href:\"#model-testing--comparison\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Model Testing & Comparison\"]}),(0,e.jsx)(n.p,{children:\"I experimented with both local Ollama models and OpenAI's GPT models:\"}),(0,e.jsxs)(n.ul,{children:[(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Local models:\"}),\" \",(0,e.jsx)(n.code,{children:\"llama3.2:latest\"}),\" and \",(0,e.jsx)(n.code,{children:\"llama3.1:8b\"})]}),(0,e.jsxs)(n.li,{children:[(0,e.jsx)(n.strong,{children:\"Cloud models:\"}),\" \",(0,e.jsx)(n.code,{children:\"gpt-4o-mini\"}),\" and \",(0,e.jsx)(n.code,{children:\"gpt-4o\"})]})]}),(0,e.jsx)(n.p,{children:\"While the appeal of local models is undeniable\\u2014zero cost, complete privacy, and no rate limits\\u2014the reality proved more complex in practice.\"}),(0,e.jsxs)(n.h3,{className:\"content-header\",id:\"the-code-interpreter-challenge\",children:[(0,e.jsx)(n.a,{href:\"#the-code-interpreter-challenge\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"The Code Interpreter Challenge\"]}),(0,e.jsx)(n.p,{children:\"The most significant issue I encountered was with tool calling, specifically the Code Interpreter tool that validates generated Python code within the Docker sandbox. This tool is critical for ensuring the AI-generated code actually works before committing it to the output folder.\"}),(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"The problem:\"}),\" Across my testing sessions, the Code Interpreter experienced 24 failed execution attempts when using local Llama models. These failures stemmed from various issues:\"]}),(0,e.jsxs)(n.ul,{children:[(0,e.jsx)(n.li,{children:\"Syntax errors in generated code that the local models didn't catch\"}),(0,e.jsx)(n.li,{children:\"Dependency misconfiguration (missing imports, incorrect package versions)\"}),(0,e.jsx)(n.li,{children:\"Docker connection issues triggered by malformed execution commands\"}),(0,e.jsx)(n.li,{children:\"Edge cases in code generation that produced valid-looking but non-functional Python\"})]}),(0,e.jsxs)(n.h3,{className:\"content-header\",id:\"quality-differences-local-vs-cloud\",children:[(0,e.jsx)(n.a,{href:\"#quality-differences-local-vs-cloud\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Quality Differences: Local vs Cloud\"]}),(0,e.jsxs)(n.p,{children:[\"After extensive testing, I consistently found that \",(0,e.jsx)(n.strong,{children:\"GPT-4o and GPT-4o-mini produced significantly higher-quality output projects\"}),\". The differences were notable across multiple dimensions:\"]}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.strong,{children:\"Code Quality:\"})}),(0,e.jsxs)(n.ul,{children:[(0,e.jsx)(n.li,{children:\"GPT models generated cleaner, more idiomatic Python\"}),(0,e.jsx)(n.li,{children:\"Better adherence to the specified architecture patterns\"}),(0,e.jsx)(n.li,{children:\"Fewer syntax errors and runtime exceptions\"})]}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.strong,{children:\"Tool Calling Reliability:\"})}),(0,e.jsxs)(n.ul,{children:[(0,e.jsx)(n.li,{children:\"GPT models successfully used the Code Interpreter tool with minimal failures\"}),(0,e.jsx)(n.li,{children:\"More accurate Docker command generation\"}),(0,e.jsx)(n.li,{children:\"Better understanding of when to validate code vs when to proceed\"})]}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.strong,{children:\"Project Completeness:\"})}),(0,e.jsxs)(n.ul,{children:[(0,e.jsx)(n.li,{children:\"GPT-generated projects required fewer manual fixes\"}),(0,e.jsx)(n.li,{children:\"More comprehensive test coverage\"}),(0,e.jsx)(n.li,{children:\"Better integration between backend, frontend, and test modules\"})]}),(0,e.jsxs)(n.h3,{className:\"content-header\",id:\"the-tradeoff-decision\",children:[(0,e.jsx)(n.a,{href:\"#the-tradeoff-decision\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"The Tradeoff Decision\"]}),(0,e.jsx)(n.p,{children:\"This experience highlighted a fundamental tradeoff in AI-powered development tools:\"}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.strong,{children:\"Local Models (Llama):\"})}),(0,e.jsxs)(n.ul,{children:[(0,e.jsx)(n.li,{children:\"\\u2705 Zero cost, complete privacy\"}),(0,e.jsx)(n.li,{children:\"\\u2705 No rate limits or API dependencies\"}),(0,e.jsx)(n.li,{children:\"\\u2705 Fast iteration for simple tasks\"}),(0,e.jsx)(n.li,{children:\"\\u274C Higher failure rates on complex tool usage\"}),(0,e.jsx)(n.li,{children:\"\\u274C More manual intervention required\"})]}),(0,e.jsx)(n.p,{children:(0,e.jsx)(n.strong,{children:\"Cloud Models (GPT-4o/4o-mini):\"})}),(0,e.jsxs)(n.ul,{children:[(0,e.jsx)(n.li,{children:\"\\u2705 Superior code quality and reliability\"}),(0,e.jsx)(n.li,{children:\"\\u2705 Successful tool calling and validation\"}),(0,e.jsx)(n.li,{children:\"\\u2705 More production-ready outputs\"}),(0,e.jsx)(n.li,{children:\"\\u274C API costs (though 4o-mini is very affordable)\"}),(0,e.jsx)(n.li,{children:\"\\u274C Requires internet connection and API keys\"})]}),(0,e.jsx)(n.p,{children:\"For rapid prototyping and experimentation, I found the best workflow was to use GPT-4o-mini as the default, with the option to fall back to local models for privacy-sensitive projects or when iterating on prompts and configurations.\"}),(0,e.jsxs)(n.h2,{className:\"content-header\",id:\"lessons-learned\",children:[(0,e.jsx)(n.a,{href:\"#lessons-learned\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Lessons Learned\"]}),(0,e.jsxs)(n.ul,{children:[(0,e.jsx)(n.li,{children:(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Model Selection Matters:\"}),\" While Llama 3.1 8B offers decent accuracy for straightforward tasks, the quality gap becomes apparent in complex multi-agent scenarios. GPT-4o excels at tool calling and produces more reliable code, while 4o-mini offers an excellent balance of quality and cost. Local models remain valuable for privacy-critical work, but expect to invest more time in validation and debugging.\"]})}),(0,e.jsx)(n.li,{children:(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Tool Calling Is the Bottleneck:\"}),\" The Code Interpreter tool is essential for validating AI-generated code, but it's also the most failure-prone component when using smaller or less capable models. Robust error handling and retry logic are critical.\"]})}),(0,e.jsx)(n.li,{children:(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Environment Setup = Success:\"}),\" Docker sockets and .env management are crucial for safe code execution and reproducibility. One misconfigured environment variable can cascade into dozens of failed tool calls.\"]})}),(0,e.jsx)(n.li,{children:(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Iterate and Refine:\"}),\" Open-source projects thrive when every config, error, and user workflow is simplified for real devs. The flexibility to swap between local and cloud models made this project practical for different use cases.\"]})})]}),(0,e.jsxs)(n.h2,{className:\"content-header\",id:\"technical-stack-summary\",children:[(0,e.jsx)(n.a,{href:\"#technical-stack-summary\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Technical Stack Summary\"]}),(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"AI Models:\"}),\" llama3.2:latest, llama3.1:8b (Ollama local), gpt-4o-mini, gpt-4o (OpenAI cloud)\"]}),(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Frameworks:\"}),\" CrewAI, Gradio\"]}),(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Infrastructure:\"}),\" Ollama (local LLM server), Docker (code sandbox), Python 3.12+\"]}),(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Languages:\"}),\" Python\"]}),(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Key Libraries:\"}),\" CrewAI[tools], Gradio, python-dotenv, pyyaml\"]}),(0,e.jsxs)(n.p,{children:[(0,e.jsx)(n.strong,{children:\"Config:\"}),\" YAML, .env\"]}),(0,e.jsx)(n.hr,{}),(0,e.jsxs)(n.h2,{className:\"content-header\",id:\"sources\",children:[(0,e.jsx)(n.a,{href:\"#sources\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(n.span,{className:\"content-header-link\",children:(0,e.jsxs)(n.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(n.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(n.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Sources\"]}),(0,e.jsxs)(n.ul,{children:[(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"https://github.com/ryan-griego/engineering_team_v1\",children:\"GitHub: engineering_team_v1\"})}),(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"https://github.com/joaomdmoura/CrewAI\",children:\"CrewAI\"})}),(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"https://ollama.com/\",children:\"Ollama\"})}),(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"https://www.gradio.app/\",children:\"Gradio\"})}),(0,e.jsx)(n.li,{children:(0,e.jsx)(n.a,{href:\"https://www.docker.com/\",children:\"Docker\"})})]})]})}function h(a={}){let{wrapper:n}=a.components||{};return n?(0,e.jsx)(n,{...a,children:(0,e.jsx)(s,{...a})}):s(a)}return x(b);})();\n;return Component;"
  },
  "_id": "posts/building-my-own-software-engineering-team-with-crew-ai.mdx",
  "_raw": {
    "sourceFilePath": "posts/building-my-own-software-engineering-team-with-crew-ai.mdx",
    "sourceFileName": "building-my-own-software-engineering-team-with-crew-ai.mdx",
    "sourceFileDir": "posts",
    "contentType": "mdx",
    "flattenedPath": "posts/building-my-own-software-engineering-team-with-crew-ai"
  },
  "type": "Blog",
  "readingTime": {
    "text": "6 min read",
    "minutes": 5.575,
    "time": 334500,
    "words": 1115
  },
  "slug": "building-my-own-software-engineering-team-with-crew-ai",
  "path": "posts/building-my-own-software-engineering-team-with-crew-ai",
  "filePath": "posts/building-my-own-software-engineering-team-with-crew-ai.mdx",
  "toc": [
    {
      "value": "The Challenge",
      "url": "#the-challenge-4",
      "depth": 2
    },
    {
      "value": "Why Local and Multi-Agent?",
      "url": "#why-local-and-multi-agent",
      "depth": 2
    },
    {
      "value": "The Solution",
      "url": "#the-solution-1",
      "depth": 2
    },
    {
      "value": "1. CrewAI Multi-Agent Coordination",
      "url": "#1-crewai-multi-agent-coordination",
      "depth": 3
    },
    {
      "value": "2. Local LLMs via Ollama",
      "url": "#2-local-llms-via-ollama",
      "depth": 3
    },
    {
      "value": "3. Interactive Requirement Selection",
      "url": "#3-interactive-requirement-selection",
      "depth": 3
    },
    {
      "value": "4. Dynamic Output Generation",
      "url": "#4-dynamic-output-generation",
      "depth": 3
    },
    {
      "value": "5. Dockerized Code Execution",
      "url": "#5-dockerized-code-execution",
      "depth": 3
    },
    {
      "value": "Key Features",
      "url": "#key-features-1",
      "depth": 2
    },
    {
      "value": "Real-World Experience: Model Performance & Challenges",
      "url": "#real-world-experience-model-performance--challenges",
      "depth": 2
    },
    {
      "value": "Model Testing & Comparison",
      "url": "#model-testing--comparison",
      "depth": 3
    },
    {
      "value": "The Code Interpreter Challenge",
      "url": "#the-code-interpreter-challenge",
      "depth": 3
    },
    {
      "value": "Quality Differences: Local vs Cloud",
      "url": "#quality-differences-local-vs-cloud",
      "depth": 3
    },
    {
      "value": "The Tradeoff Decision",
      "url": "#the-tradeoff-decision",
      "depth": 3
    },
    {
      "value": "Lessons Learned",
      "url": "#lessons-learned-4",
      "depth": 2
    },
    {
      "value": "Technical Stack Summary",
      "url": "#technical-stack-summary-2",
      "depth": 2
    },
    {
      "value": "Sources",
      "url": "#sources-6",
      "depth": 2
    }
  ],
  "structuredData": {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Building My Own Software Engineering Team with CrewAI",
    "datePublished": "2025-12-09T00:00:00.000Z",
    "dateModified": "2025-12-09T00:00:00.000Z",
    "description": "How I built an AI-powered engineering team that assembles backend, frontend, and test modules on demand‚Äîusing local LLMs with Ollama, dynamic templates, and robust Docker sandboxing.",
    "image": "/static/images/twitter-card.png",
    "url": "https://ryangriego.com/blog/posts/building-my-own-software-engineering-team-with-crew-ai"
  }
}