{
  "title": "How I Started Running DeepSeek AI Locally",
  "date": "2025-02-20T00:00:00.000Z",
  "tags": [
    "AI",
    "LLM"
  ],
  "draft": false,
  "summary": "Running the open-source DeepSeek LLM locally on your machine.",
  "body": {
    "raw": "\n# Running DeepSeek Locally: A Privacy-First Approach to AI\n\nIn an era where data privacy is important, the ability to run powerful language models locally has become increasingly valuable. Recently, I've been experimenting with DeepSeek, an impressive open-source LLM, on my M4 Mac Mini.\n\n## Getting Started with Ollama\n\nThe journey begins with Ollama, which you can think of as the npm for Large Language Models. This package manager makes the process of running LLMs locally surprisingly straightforward. Installing Ollama is as simple as visiting their website, downloading the appropriate version for your operating system, and following the installation instructions.\n\nTo verify your installation, open your terminal and run:\n\n```bash\nollama -v\n```\n\nThis command should display your installed Ollama version, confirming that everything is set up correctly.\n\n## Choosing and Installing DeepSeek\n\nOne of DeepSeek's strengths is its variety of distilled models, each optimized for different hardware constraints. You can browse through the available variations on their documentation page, selecting the one that best matches your storage capacity and performance needs.\n\nFor my setup, I opted for the 7B parameter model, which offers a good balance between capability and resource usage. I’m running it on the 16gb model of the M4 Mac mini. To install it, I simply ran:\n\n```bash\nollama run deepseek-r1:7b\n```\n\nThe beauty of this process lies in its simplicity – once installed, you can immediately start interacting with the model through your terminal. When you're done, pressing Ctrl+D will exit the session.\n\n## Enhancing the Experience with OpenWebUI\n\nWhile the terminal interface is functional, I wanted a more polished experience. This led me to OpenWebUI, a Docker-based solution that provides a clean, browser-based interface for interacting with the model.\n\nAssuming you have Docker installed and running, setting up OpenWebUI is straightforward. Just run this command:\n\n```bash\ndocker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway \\\n-v open-webui:/app/backend/data --name open-webui --restart always \\\nghcr.io/open-webui/open-webui:main\n```\n\nAfter execution, navigate to localhost:3000 in your browser, and you'll be greeted with a sophisticated chat interface that rivals commercial AI platforms.\n\n## Why DeepSeek Matters\n\nDeepSeek represents a significant milestone in democratizing access to advanced AI technology. Its open-source nature and ability to run locally address two critical needs in the developer community:\n\n1. Accessibility: Developers can now access world-class language models without the burden of expensive API costs or subscription fees.\n\n2. Privacy: By running locally, you maintain complete control over your data, ensuring that sensitive information never leaves your machine.\n\nThe ease with which DeepSeek can be deployed locally, combined with its impressive capabilities, makes it a compelling choice for developers who need reliable AI capabilities while maintaining data sovereignty.\n\n## Looking Forward\n\nAs AI continues to evolve, solutions like DeepSeek pave the way for a future where powerful AI tools can be both accessible and privacy-respecting. The ability to run such models locally isn't just about convenience – it's about empowering developers to build AI-powered applications without compromising on data privacy or breaking the bank.\n\nWhether you're building a prototype, working with sensitive data, or simply exploring the capabilities of modern AI, DeepSeek offers a practical, privacy-first approach to accessing advanced language models. I disconnected my ethernet cable and turned off WIFI and was pleased to see that I could run a LLM without an internet connection.\n\n## Sources\n\n- [Download Ollama](https://ollama.com/)\n- [Run DeepSeek from a Docker container using OpenWebUI](https://openwebui.com/)\n- [Docker](https://www.docker.com/)\n",
    "code": "var Component=(()=>{var p=Object.create;var r=Object.defineProperty;var m=Object.getOwnPropertyDescriptor;var u=Object.getOwnPropertyNames;var g=Object.getPrototypeOf,w=Object.prototype.hasOwnProperty;var y=(n,a)=>()=>(a||n((a={exports:{}}).exports,a),a.exports),b=(n,a)=>{for(var i in a)r(n,i,{get:a[i],enumerable:!0})},s=(n,a,i,t)=>{if(a&&typeof a==\"object\"||typeof a==\"function\")for(let l of u(a))!w.call(n,l)&&l!==i&&r(n,l,{get:()=>a[l],enumerable:!(t=m(a,l))||t.enumerable});return n};var f=(n,a,i)=>(i=n!=null?p(g(n)):{},s(a||!n||!n.__esModule?r(i,\"default\",{value:n,enumerable:!0}):i,n)),v=n=>s(r({},\"__esModule\",{value:!0}),n);var c=y((I,o)=>{o.exports=_jsx_runtime});var N={};b(N,{default:()=>d,frontmatter:()=>k});var e=f(c()),k={title:\"How I Started Running DeepSeek AI Locally\",date:\"2025-02-20\",tags:[\"AI\",\"LLM\"],draft:!1,summary:\"Running the open-source DeepSeek LLM locally on your machine.\"};function h(n){let a={a:\"a\",code:\"code\",h1:\"h1\",h2:\"h2\",li:\"li\",ol:\"ol\",p:\"p\",path:\"path\",pre:\"pre\",span:\"span\",svg:\"svg\",ul:\"ul\",...n.components};return(0,e.jsxs)(e.Fragment,{children:[(0,e.jsxs)(a.h1,{className:\"content-header\",id:\"running-deepseek-locally-a-privacy-first-approach-to-ai\",children:[(0,e.jsx)(a.a,{href:\"#running-deepseek-locally-a-privacy-first-approach-to-ai\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Running DeepSeek Locally: A Privacy-First Approach to AI\"]}),(0,e.jsx)(a.p,{children:\"In an era where data privacy is important, the ability to run powerful language models locally has become increasingly valuable. Recently, I've been experimenting with DeepSeek, an impressive open-source LLM, on my M4 Mac Mini.\"}),(0,e.jsxs)(a.h2,{className:\"content-header\",id:\"getting-started-with-ollama\",children:[(0,e.jsx)(a.a,{href:\"#getting-started-with-ollama\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Getting Started with Ollama\"]}),(0,e.jsx)(a.p,{children:\"The journey begins with Ollama, which you can think of as the npm for Large Language Models. This package manager makes the process of running LLMs locally surprisingly straightforward. Installing Ollama is as simple as visiting their website, downloading the appropriate version for your operating system, and following the installation instructions.\"}),(0,e.jsx)(a.p,{children:\"To verify your installation, open your terminal and run:\"}),(0,e.jsx)(a.pre,{className:\"language-bash\",children:(0,e.jsx)(a.code,{className:\"language-bash code-highlight\",children:(0,e.jsxs)(a.span,{className:\"code-line\",children:[\"ollama \",(0,e.jsx)(a.span,{className:\"token parameter variable\",children:\"-v\"}),`\n`]})})}),(0,e.jsx)(a.p,{children:\"This command should display your installed Ollama version, confirming that everything is set up correctly.\"}),(0,e.jsxs)(a.h2,{className:\"content-header\",id:\"choosing-and-installing-deepseek\",children:[(0,e.jsx)(a.a,{href:\"#choosing-and-installing-deepseek\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Choosing and Installing DeepSeek\"]}),(0,e.jsx)(a.p,{children:\"One of DeepSeek's strengths is its variety of distilled models, each optimized for different hardware constraints. You can browse through the available variations on their documentation page, selecting the one that best matches your storage capacity and performance needs.\"}),(0,e.jsx)(a.p,{children:\"For my setup, I opted for the 7B parameter model, which offers a good balance between capability and resource usage. I\\u2019m running it on the 16gb model of the M4 Mac mini. To install it, I simply ran:\"}),(0,e.jsx)(a.pre,{className:\"language-bash\",children:(0,e.jsx)(a.code,{className:\"language-bash code-highlight\",children:(0,e.jsx)(a.span,{className:\"code-line\",children:`ollama run deepseek-r1:7b\n`})})}),(0,e.jsx)(a.p,{children:\"The beauty of this process lies in its simplicity \\u2013 once installed, you can immediately start interacting with the model through your terminal. When you're done, pressing Ctrl+D will exit the session.\"}),(0,e.jsxs)(a.h2,{className:\"content-header\",id:\"enhancing-the-experience-with-openwebui\",children:[(0,e.jsx)(a.a,{href:\"#enhancing-the-experience-with-openwebui\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Enhancing the Experience with OpenWebUI\"]}),(0,e.jsx)(a.p,{children:\"While the terminal interface is functional, I wanted a more polished experience. This led me to OpenWebUI, a Docker-based solution that provides a clean, browser-based interface for interacting with the model.\"}),(0,e.jsx)(a.p,{children:\"Assuming you have Docker installed and running, setting up OpenWebUI is straightforward. Just run this command:\"}),(0,e.jsx)(a.pre,{className:\"language-bash\",children:(0,e.jsxs)(a.code,{className:\"language-bash code-highlight\",children:[(0,e.jsxs)(a.span,{className:\"code-line\",children:[(0,e.jsx)(a.span,{className:\"token function\",children:\"docker\"}),\" run \",(0,e.jsx)(a.span,{className:\"token parameter variable\",children:\"-d\"}),\" \",(0,e.jsx)(a.span,{className:\"token parameter variable\",children:\"-p\"}),\" \",(0,e.jsx)(a.span,{className:\"token number\",children:\"3000\"}),\":8080 --add-host\",(0,e.jsx)(a.span,{className:\"token operator\",children:\"=\"}),\"host.docker.internal:host-gateway \",(0,e.jsx)(a.span,{className:\"token punctuation\",children:\"\\\\\"}),`\n`]}),(0,e.jsxs)(a.span,{className:\"code-line\",children:[(0,e.jsx)(a.span,{className:\"token parameter variable\",children:\"-v\"}),\" open-webui:/app/backend/data \",(0,e.jsx)(a.span,{className:\"token parameter variable\",children:\"--name\"}),\" open-webui \",(0,e.jsx)(a.span,{className:\"token parameter variable\",children:\"--restart\"}),\" always \",(0,e.jsx)(a.span,{className:\"token punctuation\",children:\"\\\\\"}),`\n`]}),(0,e.jsx)(a.span,{className:\"code-line\",children:`ghcr.io/open-webui/open-webui:main\n`})]})}),(0,e.jsx)(a.p,{children:\"After execution, navigate to localhost:3000 in your browser, and you'll be greeted with a sophisticated chat interface that rivals commercial AI platforms.\"}),(0,e.jsxs)(a.h2,{className:\"content-header\",id:\"why-deepseek-matters\",children:[(0,e.jsx)(a.a,{href:\"#why-deepseek-matters\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Why DeepSeek Matters\"]}),(0,e.jsx)(a.p,{children:\"DeepSeek represents a significant milestone in democratizing access to advanced AI technology. Its open-source nature and ability to run locally address two critical needs in the developer community:\"}),(0,e.jsxs)(a.ol,{children:[(0,e.jsx)(a.li,{children:(0,e.jsx)(a.p,{children:\"Accessibility: Developers can now access world-class language models without the burden of expensive API costs or subscription fees.\"})}),(0,e.jsx)(a.li,{children:(0,e.jsx)(a.p,{children:\"Privacy: By running locally, you maintain complete control over your data, ensuring that sensitive information never leaves your machine.\"})})]}),(0,e.jsx)(a.p,{children:\"The ease with which DeepSeek can be deployed locally, combined with its impressive capabilities, makes it a compelling choice for developers who need reliable AI capabilities while maintaining data sovereignty.\"}),(0,e.jsxs)(a.h2,{className:\"content-header\",id:\"looking-forward\",children:[(0,e.jsx)(a.a,{href:\"#looking-forward\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Looking Forward\"]}),(0,e.jsx)(a.p,{children:\"As AI continues to evolve, solutions like DeepSeek pave the way for a future where powerful AI tools can be both accessible and privacy-respecting. The ability to run such models locally isn't just about convenience \\u2013 it's about empowering developers to build AI-powered applications without compromising on data privacy or breaking the bank.\"}),(0,e.jsx)(a.p,{children:\"Whether you're building a prototype, working with sensitive data, or simply exploring the capabilities of modern AI, DeepSeek offers a practical, privacy-first approach to accessing advanced language models. I disconnected my ethernet cable and turned off WIFI and was pleased to see that I could run a LLM without an internet connection.\"}),(0,e.jsxs)(a.h2,{className:\"content-header\",id:\"sources\",children:[(0,e.jsx)(a.a,{href:\"#sources\",\"aria-hidden\":\"true\",tabIndex:\"-1\",children:(0,e.jsx)(e.Fragment,{children:(0,e.jsx)(a.span,{className:\"content-header-link\",children:(0,e.jsxs)(a.svg,{className:\"h-5 linkicon w-5\",fill:\"currentColor\",viewBox:\"0 0 20 20\",xmlns:\"http://www.w3.org/2000/svg\",children:[(0,e.jsx)(a.path,{d:\"M12.232 4.232a2.5 2.5 0 0 1 3.536 3.536l-1.225 1.224a.75.75 0 0 0 1.061 1.06l1.224-1.224a4 4 0 0 0-5.656-5.656l-3 3a4 4 0 0 0 .225 5.865.75.75 0 0 0 .977-1.138 2.5 2.5 0 0 1-.142-3.667l3-3Z\"}),(0,e.jsx)(a.path,{d:\"M11.603 7.963a.75.75 0 0 0-.977 1.138 2.5 2.5 0 0 1 .142 3.667l-3 3a2.5 2.5 0 0 1-3.536-3.536l1.225-1.224a.75.75 0 0 0-1.061-1.06l-1.224 1.224a4 4 0 1 0 5.656 5.656l3-3a4 4 0 0 0-.225-5.865Z\"})]})})})}),\"Sources\"]}),(0,e.jsxs)(a.ul,{children:[(0,e.jsx)(a.li,{children:(0,e.jsx)(a.a,{href:\"https://ollama.com/\",children:\"Download Ollama\"})}),(0,e.jsx)(a.li,{children:(0,e.jsx)(a.a,{href:\"https://openwebui.com/\",children:\"Run DeepSeek from a Docker container using OpenWebUI\"})}),(0,e.jsx)(a.li,{children:(0,e.jsx)(a.a,{href:\"https://www.docker.com/\",children:\"Docker\"})})]})]})}function d(n={}){let{wrapper:a}=n.components||{};return a?(0,e.jsx)(a,{...n,children:(0,e.jsx)(h,{...n})}):h(n)}return v(N);})();\n;return Component;"
  },
  "_id": "posts/how-i-started-running-deepseek-ai-locally.mdx",
  "_raw": {
    "sourceFilePath": "posts/how-i-started-running-deepseek-ai-locally.mdx",
    "sourceFileName": "how-i-started-running-deepseek-ai-locally.mdx",
    "sourceFileDir": "posts",
    "contentType": "mdx",
    "flattenedPath": "posts/how-i-started-running-deepseek-ai-locally"
  },
  "type": "Blog",
  "readingTime": {
    "text": "3 min read",
    "minutes": 2.82,
    "time": 169200,
    "words": 564
  },
  "slug": "how-i-started-running-deepseek-ai-locally",
  "path": "posts/how-i-started-running-deepseek-ai-locally",
  "filePath": "posts/how-i-started-running-deepseek-ai-locally.mdx",
  "toc": [
    {
      "value": "Running DeepSeek Locally: A Privacy-First Approach to AI",
      "url": "#running-deepseek-locally-a-privacy-first-approach-to-ai",
      "depth": 1
    },
    {
      "value": "Getting Started with Ollama",
      "url": "#getting-started-with-ollama",
      "depth": 2
    },
    {
      "value": "Choosing and Installing DeepSeek",
      "url": "#choosing-and-installing-deepseek",
      "depth": 2
    },
    {
      "value": "Enhancing the Experience with OpenWebUI",
      "url": "#enhancing-the-experience-with-openwebui",
      "depth": 2
    },
    {
      "value": "Why DeepSeek Matters",
      "url": "#why-deepseek-matters",
      "depth": 2
    },
    {
      "value": "Looking Forward",
      "url": "#looking-forward",
      "depth": 2
    },
    {
      "value": "Sources",
      "url": "#sources-9",
      "depth": 2
    }
  ],
  "structuredData": {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "How I Started Running DeepSeek AI Locally",
    "datePublished": "2025-02-20T00:00:00.000Z",
    "dateModified": "2025-02-20T00:00:00.000Z",
    "description": "Running the open-source DeepSeek LLM locally on your machine.",
    "image": "/static/images/twitter-card.png",
    "url": "https://ryangriego.com/blog/posts/how-i-started-running-deepseek-ai-locally"
  }
}